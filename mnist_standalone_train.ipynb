{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Master and Slave\n",
    "\n",
    "Please run the following commands in terminal to start master and slave on every machine using user hdfs. You can look up the spark home env in the env.py.\n",
    "\n",
    "```bash\n",
    "su hdfs\n",
    "export MASTER=spark://master0.datascience.com:7077\n",
    "export CORES_PER_WORKER=1\n",
    "% on master machine, run:\n",
    "${SPARK_HOME}/sbin/start-master.sh;\n",
    "% on slave machine, run:\n",
    "${SPARK_HOME}/sbin/start-slave.sh -c ${CORES_PER_WORKER} -m 3G ${MASTER}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment\n",
    "\n",
    "The codes in env.py do not have to be changed for a different training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "# %env # print env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Please import and initialize findspark first before import other pyspark libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflowonspark import TFCluster\n",
    "from argparse import Namespace\n",
    "\n",
    "from mnist_train_tf import map_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark Config\n",
    "\n",
    "Our machine is running on spark 2.3.\n",
    "Spark env properties can be found [here](https://spark.apache.org/docs/2.3.0/configuration.html).\n",
    "Yarn env properties can be found [here](https://spark.apache.org/docs/2.3.0/running-on-yarn.html).\n",
    "\n",
    "Some notes:\n",
    "\n",
    "**spark.submit.deployMode**: please change the value from `client` to `cluster` if you submit your work from different location, details can be found [here](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use?rq=1).\n",
    "\n",
    "**spark.executor.instances**: this should match the amount of your worker nodes.\n",
    "\n",
    "**spark.executor.memory**: check the remaining memory first at YARN ResourceManager UI, you may want to kill some dead process first.\n",
    "\n",
    "**spark.executorEnv.CLASSPATH**: specifies the location of user-defined classes and packages, sometimes this is used for backward compatiblity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "conf.setAll([(\"spark.app.name\", \"mnist-standalone-train\"), # your app name\n",
    "             (\"spark.master\", \"spark://master0.datascience.com:7077\"), # cluster mode, please leave this unchanged\n",
    "             (\"spark.cores.max\", \"2\"), # check comments above\n",
    "             (\"spark.task.cpus\", \"1\"), # check comments above\n",
    "             (\"spark.executorEnv.LD_LIBRARY_PATH\", os.environ[\"LD_LIBRARY_PATH\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.HADOOP_HDFS_HOME\", os.environ[\"HADOOP_HDFS_HOME\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.CLASSPATH\", os.environ[\"CLASSPATH\"])]) # please leave this unchanged\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.addPyFile(\"mnist_train_tf.py\")\n",
    "\n",
    "# print(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "  batch_size=100, # number of records per batch\n",
    "  epochs=1, # number of epochs\n",
    "  format=\"csv\", # example format: [\"csv\", \"tfr\"]\n",
    "  images=\"/data/mnist/csv/train/images\", # HDFS path to MNIST images in parallelized format\n",
    "  labels=\"/data/mnist/csv/train/labels\", # HDFS path to MNIST labels in parallelized format\n",
    "  model=\"/data/mnist/model\", # HDFS path to save/load model during train/inference\n",
    "  cluster_size=2, # number of nodes in the cluster\n",
    "  num_ps=1, # number of parameter servers\n",
    "  output=\"/data/mnist/predictions\", # HDFS path to save test/inference output\n",
    "  readers=1, # number of reader/enqueue threads\n",
    "  steps=1000, # maximum number of steps\n",
    "  tensorboard=False, # launch tensorboard process\n",
    "  mode=\"train\", # train|inference\n",
    "  rdma=False # use rdma connection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-16 01:14:53,903 INFO (MainThread-39145) Reserving TFSparkNodes \n",
      "2019-03-16 01:14:53,904 INFO (MainThread-39145) cluster_template: {'ps': range(0, 1), 'worker': range(1, 2)}\n",
      "2019-03-16 01:14:53,908 INFO (MainThread-39145) listening for reservations at ('172.29.0.3', 37654)\n",
      "2019-03-16 01:14:53,909 INFO (MainThread-39145) Starting TensorFlow on executors\n",
      "2019-03-16 01:14:53,917 INFO (MainThread-39145) Waiting for TFSparkNodes to start\n",
      "2019-03-16 01:14:53,918 INFO (MainThread-39145) waiting for 2 reservations\n",
      "2019-03-16 01:14:54,920 INFO (MainThread-39145) waiting for 2 reservations\n",
      "2019-03-16 01:14:55,922 INFO (MainThread-39145) waiting for 2 reservations\n",
      "2019-03-16 01:14:56,923 INFO (MainThread-39145) all reservations completed\n",
      "2019-03-16 01:14:56,925 INFO (MainThread-39145) All TFSparkNodes started\n",
      "2019-03-16 01:14:56,925 INFO (MainThread-39145) {'executor_id': 1, 'host': '172.29.0.5', 'job_name': 'worker', 'task_index': 0, 'port': 42014, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-g3xhr64a/listener-0a7cgias', 'authkey': b'\\n\\xd1$VD#E\\xf2\\x91\\xf1N\\x0cMi\\xa3\\xd2'}\n",
      "2019-03-16 01:14:56,926 INFO (MainThread-39145) {'executor_id': 0, 'host': '172.29.0.6', 'job_name': 'ps', 'task_index': 0, 'port': 36988, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.29.0.6', 33564), 'authkey': b'ey<\\xc2\\x9a\\xaaL\\x18\\x98\\x99\\xc18J0\\xf1\\xed'}\n",
      "2019-03-16 01:14:56,927 INFO (MainThread-39145) Feeding training data\n",
      "2019-03-16 01:18:15,407 INFO (MainThread-39145) Stopping TensorFlow nodes\n",
      "2019-03-16 01:18:15,526 INFO (MainThread-39145) Shutting down cluster\n"
     ]
    }
   ],
   "source": [
    "if args.format == \"tfr\":\n",
    "  images = sc.newAPIHadoopFile(args.images, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n",
    "                               keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                               valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "\n",
    "  def toNumpy(bytestr):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(bytestr)\n",
    "    features = example.features.feature\n",
    "    image = numpy.array(features['image'].int64_list.value)\n",
    "    label = numpy.array(features['label'].int64_list.value)\n",
    "    return (image, label)\n",
    "\n",
    "  dataRDD = images.map(lambda x: toNumpy(bytes(x[0])))\n",
    "else:  # args.format == \"csv\":\n",
    "  images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "  labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "  dataRDD = images.zip(labels)\n",
    "\n",
    "cluster = TFCluster.run(sc, map_fun, args, args.cluster_size, args.num_ps, args.tensorboard, TFCluster.InputMode.SPARK)\n",
    "if args.mode == \"train\":\n",
    "  cluster.train(dataRDD, args.epochs)\n",
    "else:\n",
    "  labelRDD = cluster.inference(dataRDD)\n",
    "  labelRDD.saveAsTextFile(args.output)\n",
    "cluster.shutdown()\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
