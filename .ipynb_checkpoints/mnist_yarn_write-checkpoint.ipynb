{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Environment\n",
    "\n",
    "The codes in env.py do not have to be changed for a different training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "# %env # print env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Please import and initialize findspark first before import other pyspark libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from mnist_write import writeMNIST, readMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark Config\n",
    "\n",
    "Our machine is running on spark 2.3.\n",
    "Spark env properties can be found [here](https://spark.apache.org/docs/2.3.0/configuration.html).\n",
    "Yarn env properties can be found [here](https://spark.apache.org/docs/2.3.0/running-on-yarn.html#configuration).\n",
    "\n",
    "Some notes:\n",
    "\n",
    "**spark.submit.deployMode**: please change the value from `client` to `cluster` if you submit your work from different location, details can be found [here](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use?rq=1).\n",
    "\n",
    "**spark.executor.instances**: this should match the amount of your worker nodes.\n",
    "\n",
    "**spark.executor.memory**: check the remaining memory first at YARN ResourceManager UI, you may want to kill some dead process first.\n",
    "\n",
    "**spark.executorEnv.CLASSPATH**: specifies the location of user-defined classes and packages, sometimes this is used for backward compatiblity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "conf.setAll([(\"spark.app.name\", \"mnist-yarn-write\"), # your app name\n",
    "             (\"spark.master\", \"yarn\"), # cluster mode, please leave this unchanged\n",
    "             (\"spark.yarn.queue\", \"default\"), # value=default for CPU only, value=gpu if GPU enabled\n",
    "             (\"spark.yarn.maxAppAttempts\", \"1\"), # please leave this unchanged\n",
    "             (\"spark.submit.deployMode\", \"client\"), # check comments above\n",
    "             (\"spark.executor.instances\", \"2\"), # check comments above\n",
    "             (\"spark.executor.memory\", \"4G\"), # check comments above\n",
    "             (\"spark.dynamicAllocation.enabled\", \"false\"), # please leave this unchanged\n",
    "             (\"spark.executorEnv.LD_LIBRARY_PATH\", os.environ[\"LD_LIBRARY_PATH\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.HADOOP_HDFS_HOME\", os.environ[\"HADOOP_HDFS_HOME\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.CLASSPATH\", os.environ[\"CLASSPATH\"])]) # please leave this unchanged\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.addPyFile(\"mnist_write.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "  format=\"csv\", # output format: [\"csv\", \"csv2\", \"pickle\", \"tf\", \"tfr\"]\n",
    "  num_partitions=10, # number of output partitions\n",
    "  output=\"/data/mnist/csv\", # HDFS directory to save examples in parallelized format\n",
    "  read=False, # read previously saved examples\n",
    "  verify=True # verify saved examples after writing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.read:\n",
    "  # Note: these files are inside the mnist.zip file\n",
    "  writeMNIST(sc, \"data/train-images-idx3-ubyte.gz\", \"data/train-labels-idx1-ubyte.gz\", args.output + \"/train\", args.format, args.num_partitions)\n",
    "  writeMNIST(sc, \"data/t10k-images-idx3-ubyte.gz\", \"data/t10k-labels-idx1-ubyte.gz\", args.output + \"/test\", args.format, args.num_partitions)\n",
    "\n",
    "if args.read or args.verify:\n",
    "  readMNIST(sc, args.output + \"/train\", args.format)\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
