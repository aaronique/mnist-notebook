{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment\n",
    "\n",
    "The codes in env.py do not have to be changed for a different training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "# %env # print env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Please import and initialize findspark first before import other pyspark libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflowonspark import TFCluster\n",
    "from argparse import Namespace\n",
    "\n",
    "from mnist_train_tf import map_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark Config\n",
    "\n",
    "Our machine is running on spark 2.3.\n",
    "Spark env properties can be found [here](https://spark.apache.org/docs/2.3.0/configuration.html).\n",
    "Yarn env properties can be found [here](https://spark.apache.org/docs/2.3.0/running-on-yarn.html).\n",
    "\n",
    "Some notes:\n",
    "\n",
    "**spark.submit.deployMode**: please change the value from `client` to `cluster` if you submit your work from different location, details can be found [here](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use?rq=1).\n",
    "\n",
    "**spark.executor.instances**: this should match the amount of your worker nodes.\n",
    "\n",
    "**spark.executor.memory**: check the remaining memory first at YARN ResourceManager UI, you may want to kill some dead process first.\n",
    "\n",
    "**spark.executorEnv.CLASSPATH**: specifies the location of user-defined classes and packages, sometimes this is used for backward compatiblity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "conf.setAll([(\"spark.app.name\", \"mnist-yarn-train\"), # your app name\n",
    "             (\"spark.master\", \"yarn\"), # cluster mode, please leave this unchanged\n",
    "             (\"spark.yarn.queue\", \"default\"), # value=default for CPU only, value=gpu if GPU enabled\n",
    "             (\"spark.yarn.maxAppAttempts\", \"1\"), # please leave this unchanged\n",
    "             (\"spark.submit.deployMode\", \"client\"), # check comments above\n",
    "             (\"spark.executor.instances\", \"2\"), # check comments above\n",
    "             (\"spark.executor.memory\", \"3G\"), # check comments above\n",
    "             (\"spark.dynamicAllocation.enabled\", \"false\"), # please leave this unchanged\n",
    "             (\"spark.executorEnv.LD_LIBRARY_PATH\", os.environ[\"LD_LIBRARY_PATH\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.HADOOP_HDFS_HOME\", os.environ[\"HADOOP_HDFS_HOME\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.CLASSPATH\", os.environ[\"CLASSPATH\"])]) # please leave this unchanged\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.addPyFile(\"mnist_train_tf.py\")\n",
    "\n",
    "# print(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "  batch_size=100, # number of records per batch\n",
    "  epochs=1, # number of epochs\n",
    "  format=\"csv\", # example format: [\"csv\", \"tfr\"]\n",
    "  images=\"/data/mnist/csv/train/images\", # HDFS path to MNIST images in parallelized format\n",
    "  labels=\"/data/mnist/csv/train/labels\", # HDFS path to MNIST labels in parallelized format\n",
    "  model=\"/data/mnist/model\", # HDFS path to save/load model during train/inference\n",
    "  cluster_size=2, # number of nodes in the cluster\n",
    "  num_ps=1, # number of parameter servers\n",
    "  output=\"/data/mnist/predictions\", # HDFS path to save test/inference output\n",
    "  readers=1, # number of reader/enqueue threads\n",
    "  steps=1000, # maximum number of steps\n",
    "  tensorboard=True, # launch tensorboard process\n",
    "  mode=\"train\", # train|inference\n",
    "  rdma=False # use rdma connection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 21:39:34,950 INFO (MainThread-97634) Reserving TFSparkNodes w/ TensorBoard\n",
      "2019-03-15 21:39:34,951 INFO (MainThread-97634) cluster_template: {'ps': range(0, 1), 'worker': range(1, 2)}\n",
      "2019-03-15 21:39:34,954 INFO (MainThread-97634) listening for reservations at ('172.29.0.3', 46679)\n",
      "2019-03-15 21:39:34,955 INFO (MainThread-97634) Starting TensorFlow on executors\n",
      "2019-03-15 21:39:34,962 INFO (MainThread-97634) Waiting for TFSparkNodes to start\n",
      "2019-03-15 21:39:34,964 INFO (MainThread-97634) waiting for 2 reservations\n",
      "2019-03-15 21:39:35,967 INFO (MainThread-97634) waiting for 2 reservations\n",
      "2019-03-15 21:39:36,969 INFO (MainThread-97634) waiting for 2 reservations\n",
      "2019-03-15 21:39:37,971 INFO (MainThread-97634) all reservations completed\n",
      "2019-03-15 21:39:37,972 INFO (MainThread-97634) All TFSparkNodes started\n",
      "2019-03-15 21:39:37,972 INFO (MainThread-97634) {'executor_id': 1, 'host': '172.29.0.5', 'job_name': 'worker', 'task_index': 0, 'port': 38519, 'tb_pid': 88175, 'tb_port': 35499, 'addr': '/tmp/pymp-8efebyig/listener-2kw6o3ts', 'authkey': b'\\x021\\xe0\\x1b\\x1c\\x1fI3\\xb2E\\xbb\\xe1\\xcc\\x9b\\x1d\\xa6'}\n",
      "2019-03-15 21:39:37,973 INFO (MainThread-97634) {'executor_id': 0, 'host': '172.29.0.6', 'job_name': 'ps', 'task_index': 0, 'port': 35693, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.29.0.6', 34867), 'authkey': b'\\x11>\\xd8\\xf9\\xa7\\xb3A[\\xa8\\xfeLH\\xbdhj\\xab'}\n",
      "2019-03-15 21:39:37,974 INFO (MainThread-97634) ========================================================================================\n",
      "2019-03-15 21:39:37,974 INFO (MainThread-97634) \n",
      "2019-03-15 21:39:37,975 INFO (MainThread-97634) TensorBoard running at:       http://172.29.0.5:35499\n",
      "2019-03-15 21:39:37,975 INFO (MainThread-97634) \n",
      "2019-03-15 21:39:37,976 INFO (MainThread-97634) ========================================================================================\n",
      "2019-03-15 21:39:37,977 INFO (MainThread-97634) Feeding training data\n",
      "2019-03-15 21:42:56,245 INFO (MainThread-97634) Stopping TensorFlow nodes\n",
      "2019-03-15 21:42:56,365 INFO (MainThread-97634) Shutting down cluster\n"
     ]
    }
   ],
   "source": [
    "if args.format == \"tfr\":\n",
    "  images = sc.newAPIHadoopFile(args.images, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n",
    "                               keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                               valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "  def toNumpy(bytestr):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(bytestr)\n",
    "    features = example.features.feature\n",
    "    image = numpy.array(features['image'].int64_list.value)\n",
    "    label = numpy.array(features['label'].int64_list.value)\n",
    "    return (image, label)\n",
    "  dataRDD = images.map(lambda x: toNumpy(bytes(x[0])))\n",
    "\n",
    "else:  # args.format == \"csv\":\n",
    "  images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "  labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "  dataRDD = images.zip(labels)\n",
    "\n",
    "cluster = TFCluster.run(sc, map_fun, args, args.cluster_size, args.num_ps, args.tensorboard, TFCluster.InputMode.SPARK)\n",
    "\n",
    "if args.mode == \"train\":\n",
    "  cluster.train(dataRDD, args.epochs)\n",
    "\n",
    "else:\n",
    "  labelRDD = cluster.inference(dataRDD)\n",
    "  labelRDD.saveAsTextFile(args.output)\n",
    "cluster.shutdown()\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
