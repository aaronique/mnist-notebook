{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment\n",
    "\n",
    "The codes in env.py do not have to be changed for a different training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env.py\n",
    "# %env # print env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Please import and initialize findspark first before import other pyspark libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mnist_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-acc1f5d294a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0margparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmnist_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mnist_train'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflowonspark import TFCluster\n",
    "from argparse import Namespace\n",
    "\n",
    "from mnist_train_tf import map_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark Config\n",
    "\n",
    "Our machine is running on spark 2.3.\n",
    "Spark env properties can be found [here](https://spark.apache.org/docs/2.3.0/configuration.html).\n",
    "Yarn env properties can be found [here](https://spark.apache.org/docs/2.3.0/running-on-yarn.html#configuration).\n",
    "\n",
    "Some notes:\n",
    "\n",
    "**spark.submit.deployMode**: please change the value from `client` to `cluster` if you submit your work from different location, details can be found [here](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use?rq=1).\n",
    "\n",
    "**spark.executor.instances**: this should match the amount of your worker nodes.\n",
    "\n",
    "**spark.executor.memory**: check the remaining memory first at YARN ResourceManager UI, you may want to kill some dead process first.\n",
    "\n",
    "**spark.executorEnv.CLASSPATH**: specifies the location of user-defined classes and packages, sometimes this is used for backward compatiblity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "conf.setAll([(\"spark.app.name\", \"mnist-yarn-train\"), # your app name\n",
    "             (\"spark.master\", \"yarn\"), # cluster mode, please leave this unchanged\n",
    "             (\"spark.yarn.queue\", \"default\"), # value=default for CPU only, value=gpu if GPU enabled\n",
    "             (\"spark.yarn.maxAppAttempts\", \"1\"), # please leave this unchanged\n",
    "             (\"spark.submit.deployMode\", \"client\"), # check comments above\n",
    "             (\"spark.executor.instances\", \"2\"), # check comments above\n",
    "             (\"spark.executor.memory\", \"3G\"), # check comments above\n",
    "             (\"spark.dynamicAllocation.enabled\", \"false\"), # please leave this unchanged\n",
    "             (\"spark.executorEnv.LD_LIBRARY_PATH\", os.environ[\"LD_LIBRARY_PATH\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.HADOOP_HDFS_HOME\", os.environ[\"HADOOP_HDFS_HOME\"]), # please leave this unchanged\n",
    "             (\"spark.executorEnv.CLASSPATH\", os.environ[\"CLASSPATH\"])]) # please leave this unchanged\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.addPyFile(\"mnist_train_tf.py\")\n",
    "\n",
    "# print(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "  batch_size=100, # number of records per batch\n",
    "  epochs=1, # number of epochs\n",
    "  format=\"csv\", # example format: [\"csv\", \"tfr\"]\n",
    "  images=\"/data/mnist/csv/train/images\", # HDFS path to MNIST images in parallelized format\n",
    "  labels=\"/data/mnist/csv/train/labels\", # HDFS path to MNIST labels in parallelized format\n",
    "  model=\"/data/mnist/model\", # HDFS path to save/load model during train/inference\n",
    "  cluster_size=2, # number of nodes in the cluster\n",
    "  num_ps=1, # number of parameter servers\n",
    "  output=\"/data/mnist/predictions\", # HDFS path to save test/inference output\n",
    "  readers=1, # number of reader/enqueue threads\n",
    "  steps=1000, # maximum number of steps\n",
    "  tensorboard=True, # launch tensorboard process\n",
    "  mode=\"train\", # train|inference\n",
    "  rdma=False # use rdma connection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if args.format == \"tfr\":\n",
    "  images = sc.newAPIHadoopFile(args.images, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n",
    "                               keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                               valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "  def toNumpy(bytestr):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(bytestr)\n",
    "    features = example.features.feature\n",
    "    image = numpy.array(features['image'].int64_list.value)\n",
    "    label = numpy.array(features['label'].int64_list.value)\n",
    "    return (image, label)\n",
    "  dataRDD = images.map(lambda x: toNumpy(bytes(x[0])))\n",
    "\n",
    "else:  # args.format == \"csv\":\n",
    "  images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "  labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "  dataRDD = images.zip(labels)\n",
    "\n",
    "cluster = TFCluster.run(sc, map_fun, args, args.cluster_size, args.num_ps, args.tensorboard, TFCluster.InputMode.SPARK)\n",
    "\n",
    "if args.mode == \"train\":\n",
    "  cluster.train(dataRDD, args.epochs)\n",
    "\n",
    "else:\n",
    "  labelRDD = cluster.inference(dataRDD)\n",
    "  labelRDD.saveAsTextFile(args.output)\n",
    "cluster.shutdown()\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
